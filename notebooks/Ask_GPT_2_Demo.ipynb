{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ü§ñ Ask GPT-2 Demo: Fine-Tuning for Question Answering\n",
        "\n",
        "Welcome! This demo notebook showcases my fine-tuned version of GPT-2, specialized for question answering tasks.  \n",
        "\n",
        "You‚Äôll be able to:\n",
        "\n",
        "- See how GPT-2 performs **before and after** fine-tuning\n",
        "- Try your own prompts to test both models\n",
        "- Learn what was done, and where it can go next\n",
        "\n",
        "<br>\n",
        "\n",
        "In this short project, I fine-tuned `distilgpt2` on a small dataset of conversational Q&A (`mlabonne/guanaco-llama2-1k`) to help it better handle question answering tasks, things like **\"How are you?\"** or **\"Explain what games are.\"**\n",
        "\n",
        "> You can view the question answering dataset here: [mlabonne/guanaco-llama2-1k](https://huggingface.co/datasets/mlabonne/guanaco-llama2-1k).\n",
        "\n",
        "<br>\n",
        "\n",
        "### ‚ú® What I Did\n",
        "- Fine-tuned `distilgpt2` using Hugging Face‚Äôs `trl` library\n",
        "- Used a small instruction-tuned dataset to simulate assistant-style behavior\n",
        "- Published the resulting model to the Hugging Face Hub\n",
        "\n",
        "### üí° What I Learned\n",
        "- How to take a general-purpose language model and give it more specific behaviors\n",
        "- How instruction-tuned datasets can make a huge difference in output quality\n",
        "- How to host and share models with others using the ü§ó Hub\n",
        "\n",
        "### üìù What You Can Do Here\n",
        "- Try out different prompts using both the **original GPT-2** and my **fine-tuned version**\n",
        "- See side-by-side how the model's behavior improves\n",
        "- Play around with your own questions and see how it responds!\n",
        "\n",
        "> This demo is for anyone curious about how models evolve through fine-tuning with no technical setup needed.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qsCDFiOZazRJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üí≠ Why Isn‚Äôt GPT-2 Great at Q&A by Default?\n",
        "\n",
        "GPT-2 was originally trained as a **text completion model**, that is, it's really good at continuing text like stories or articles, but not necessarily at answering questions in a helpful, coherent, or direct way.\n",
        "\n",
        "Out of the box, GPT-2 might:\n",
        "- Try to complete your question instead of answering it\n",
        "- Generate vague or off-topic responses\n",
        "- Struggle with clarity and coherence\n",
        "\n",
        "That‚Äôs where fine-tuning helps! By training it on example Q&A data, we teach GPT-2 how to act more like a helpful assistant and less like a random sentence finisher.\n",
        "\n",
        "With this being said, we can now jump into my demo by following and running the code cells below!"
      ],
      "metadata": {
        "id": "Ojeg7Gr2hAre"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''SETUP: Cloning my Repo and Loading Both Models'''\n",
        "\n",
        "# Clone my Repo\n",
        "!git clone https://github.com/Akhan521/Ask-GPT-2.git\n",
        "%cd Ask-GPT-2\n",
        "\n",
        "# Install Dependencies\n",
        "!pip install datasets\n",
        "!pip install transformers -U\n",
        "!pip install accelerate -U   # To accelerate training / Leverage multiple GPUs if available.\n",
        "!pip install torchvision\n",
        "!pip install trl             # Highly-optimized for training transformers.\n",
        "\n",
        "# Load Both Base and Fine-Tuned Models\n",
        "import torch\n",
        "from src.data_loader import get_tokenizer\n",
        "from src.model import load_model\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Base Model (distilgpt2 from Hugging Face)\n",
        "base_model_name = \"distilgpt2\"\n",
        "base_model = load_model(base_model_name)\n",
        "base_tokenizer = get_tokenizer(base_model_name)\n",
        "\n",
        "# Fine-Tuned Model (from Hugging Face Hub)\n",
        "finetuned_model_name = \"akhan365/distilgpt2-finetuned-on-guanaco-for-qa\"\n",
        "finetuned_model = load_model(finetuned_model_name)\n",
        "finetuned_tokenizer = get_tokenizer(finetuned_model_name)\n",
        "\n",
        "print(\"\\n‚úÖ Both models loaded successfully!\")\n"
      ],
      "metadata": {
        "id": "YbR2LjDZhQlH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''COMPARISON: Comparing Outputs From Both Models'''\n",
        "\n",
        "from src.generate import generate\n",
        "\n",
        "# Function to Compare Outputs\n",
        "def compare_models(prompt: str):\n",
        "    print(\"üìå Prompt:\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"{prompt}\\n\")\n",
        "\n",
        "    print(\"üß† Base GPT-2 (distilgpt2) Response:\")\n",
        "    print(\"=\" * 50)\n",
        "    base_output = generate(prompt, base_model, base_tokenizer, device=device)\n",
        "    print(base_output)\n",
        "\n",
        "    print(\"\\nü§ñ Fine-Tuned GPT-2 Response:\")\n",
        "    print(\"=\" * 50)\n",
        "    finetuned_output = generate(prompt, finetuned_model, finetuned_tokenizer, device=device)\n",
        "    print(finetuned_output)\n"
      ],
      "metadata": {
        "id": "aLUceh3DvldZ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîç Try It Yourself: Compare the Base and Fine-Tuned GPT-2 Models\n",
        "\n",
        "Use the interactive cell below to test your own prompts!\n",
        "\n",
        "- The **base model** is `distilgpt2` from Hugging Face.\n",
        "- My **fine-tuned model** was trained on the [Guanaco QA dataset](https://huggingface.co/datasets/mlabonne/guanaco-llama2-1k) to better handle **question answering** tasks.\n",
        "\n",
        "Try entering a question like **\"How are you?\"** and observe how the outputs differ.\n",
        "\n",
        "This shows how fine-tuning can steer a general language model to better handle specific tasks.\n"
      ],
      "metadata": {
        "id": "Fknqgk2PwvpX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Play around with the prompt:\n",
        "\n",
        "# If you modify this prompt, make sure to re-run this code cell!\n",
        "prompt = \"how are you\"\n",
        "\n",
        "compare_models(prompt)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBFLwZ6vxRiP",
        "outputId": "6642b040-9e5f-4949-e5c1-80da2fc71008"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìå Prompt:\n",
            "==================================================\n",
            "how are you\n",
            "\n",
            "üß† Base GPT-2 (distilgpt2) Response:\n",
            "==================================================\n",
            "how are you going to want the best?\n",
            "I've been working on a lot of great games, but I think it's just too late. So that was my first game in awhile and then something like this came out with some really interesting ideas for how we could do better together.\"<|endoftext|>\n",
            "\n",
            "ü§ñ Fine-Tuned GPT-2 Response:\n",
            "==================================================\n",
            "how are you, and how do I respond to that?\n",
            "\"It's a good idea for me not only be polite but also have an excellent understanding of the world. If there is any criticism or confusion about my position on this topic it would be best if they could explain why such comments were made.\"<|endoftext|>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üõ†Ô∏è Potential Improvements\n",
        "\n",
        "While my fine-tuned GPT-2 model performs noticeably better than the base model on question answering tasks, there are limitations and there's still plenty of room for growth:\n",
        "\n",
        "- **Relevance & Consistency**  \n",
        "  My fine-tuned model sometimes produces answers that are vague, overly verbose, or slightly off-topic. This is partly due to the limited size of the training dataset and the model's inherent limitations as a lightweight model.\n",
        "\n",
        "- **Small Model, Small Context**  \n",
        "  GPT-2 has a relatively small context window and parameter count. Larger models like GPT-Neo, Mistral, or LLaMA variants often provide better fluency and capabilities.\n",
        "\n",
        "- **Training with Techniques Like LoRA or QLoRA**  \n",
        "  My project used full fine-tuning, which is memory-intensive and slower. Techniques like **LoRA** or **QLoRA** allow faster, more efficient fine-tuning. This is especially useful on low-resource hardware or larger base models.\n",
        "\n",
        "- **Dataset Diversity**  \n",
        "  Expanding beyond a single dataset (like Guanaco) and incorporating more varied or challenging QA examples would improve my fine-tuned model's robustness and generality.\n",
        "\n",
        "\n",
        "By tackling these areas, my model can become a more helpful, accurate, and reliable assistant for question answering tasks."
      ],
      "metadata": {
        "id": "q-IcDw_hy1nw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìò Final Remarks\n",
        "\n",
        "This project taught me how even a small dataset can meaningfully steer a language model's behavior.\n",
        "\n",
        "Key takeaways:\n",
        "- GPT-2 struggles with structured Q&A because it‚Äôs a generic text completion model.\n",
        "- Fine-tuning helps specialize the model for tasks like question answering.\n",
        "- Tools like `transformers`, `trl`, and `Hugging Face Hub` make it easy to manage the whole training + inference workflow.\n",
        "\n",
        "Thanks for checking my demo out! Feel free to reach out and connect with me.\n",
        "\n",
        "- Connect with me here: [LinkedIn Profile](https://www.linkedin.com/in/aamir-khan-aak521/)\n",
        "- View my portfolio here: [Portfolio](https://aamir-khans-portfolio.vercel.app/)\n",
        "- View my code here: [Repository](https://github.com/Akhan521/Ask-GPT-2)\n"
      ],
      "metadata": {
        "id": "DhOGrUafx98J"
      }
    }
  ]
}